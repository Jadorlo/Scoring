{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TROISIEME CYCLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce troisième cycle, notre but est de prendre les meilleurs modèles Logit et Arbre de décision du deuxième cycle et de leur faire prédire la catégorie d'income des individus du fichier nouvelle_data.csv.\n",
    "\n",
    "Pour cela, nous devons d'abord mettre en forme le fichier nouvelle_data afin que celui-ci ai la même structure que le fichier data_V2. Nous utilisons la fonction Create_X_nouvelle_data pour appliquer les transformations vues précedemment.Le seul changement réside dans la construction des nouvelles variables qualitatives $\\textbf{age}$ et $\\textbf{hours-per-week}$ pour lesquelles nous avions précedemment un fichier répertoriant les classes optimisées pour les 48843 individus du fichier d'entraînement. Ici nous avons du créer une nouvelle fonction get_classes qui récupère les classes associées aux deux variables initialement quantitatives. \n",
    "\n",
    "Nous effectuons enfin la prédiction en entraînant les modèles de la même manière que les derniers modèles du deuxième cycle. Nous tirerons les prédictions de ces derniers et nous les analyserons rapidement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Train_Test(df):\n",
    "    \"\"\"\n",
    "    Créer les dataframes de test et d'entraînement \n",
    "    \"\"\"\n",
    "    y = df.pop('income')\n",
    "    y = pd.get_dummies(y)['>50K']\n",
    "    X = pd.get_dummies(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0, stratify=y)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_X_nouvelle_data():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def drop_columns(df):\n",
    "        \"\"\"\n",
    "        Drop les colonnes fnlwgt, educational-nul car inutiles\n",
    "        \"\"\"\n",
    "        df.drop([\"fnlwgt\", \"educational-num\"], inplace=True, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def get_classes(df):\n",
    "        \"\"\"\n",
    "        Applique les classes optimisées générées par le fichier optimisation_classes.py\n",
    "        \"\"\"\n",
    "\n",
    "        def classifier(classes, var):\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # Parcourir chaque intervalle dans la liste\n",
    "            for interval in classes:\n",
    "                # Extraire l'intervalle sous forme de chaîne\n",
    "                interval_str = interval[0]\n",
    "\n",
    "                # Extraire les bornes inférieure et supérieure en enlevant les parenthèses et crochets\n",
    "                lower_bound, upper_bound = interval_str[1:-1].split(', ')\n",
    "                \n",
    "                # Convertir les bornes en float\n",
    "                lower_bound = float(lower_bound)\n",
    "                upper_bound = float(upper_bound)\n",
    "\n",
    "                # Vérifier si l'âge est dans cet intervalle\n",
    "                if lower_bound < var <= upper_bound:\n",
    "                    return interval_str\n",
    "                \n",
    "        serie_age = pd.read_csv('files/classes_opti/classes_opti_alexander_age.csv')\n",
    "        serie_hpw = pd.read_csv('files/classes_opti/classes_opti_alexander_hours-per-week.csv')\n",
    "        \n",
    "        for serie in [serie_age, serie_hpw]:\n",
    "            classes = serie.value_counts().index\n",
    "            df_classes = df[serie.columns[0]].apply(lambda x : classifier(classes, x))\n",
    "            df.drop(serie.columns[0], inplace=True, axis=1)\n",
    "            df = pd.concat([df, df_classes], axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def regroupement_V2(df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Regroupement des personnes d'ethnies autre que blanc et noir en \"Other_race\"\n",
    "        df['race'] = df['race'].replace(\n",
    "            {'Asian-Pac-Islander|Amer-Indian-Eskimo|Other': 'Other_race'}, regex=True)\n",
    "\n",
    "        # Regroupement des personnes d'origines différentes en une cartégorie \"Other\"\n",
    "        df['native-country'] = df['native-country'].replace(\n",
    "            {'^(?!United-States$).+': 'Other_country'}, regex=True)\n",
    "\n",
    "        # Regroupement des personnes mariés mais avec un conjoint dans l'armé avec les personnes mariés mais avec un conjoint absent pour diverse raisons\n",
    "        df['marital-status'] = df['marital-status'].replace(\n",
    "            {'Married-AF-spouse|Divorced|Widowed|Separated|Married-spouse-absent': 'Alone'}, regex = True)\n",
    "\n",
    "        df['education'] = df['education'].replace(\n",
    "            {'Preschool|1st-4th|5th-6th|7th-8th|9th|10th|11th|12th': 'Low-education',\n",
    "            'Doctorate|Prof-school': \"Graduation\",\n",
    "            \"Assoc-acdm|Assoc-voc\" : 'Assoc'}, regex=True)\n",
    "\n",
    "        df['workclass'] = df['workclass'].replace(\n",
    "            {'Local-gov|State-gov': 'State-Local-gov',\n",
    "            'Never-worked|Without-pay|\\?' : 'No_income_or_unknown'}, regex=True)\n",
    "\n",
    "        df['occupation'] = df['occupation'].replace(\n",
    "            {'\\?|Priv-house-serv|Other-service|Handlers-cleaners': 'Occupation:Very-Low-income',\n",
    "            'Farming-fishing|Machine-op-inspct|Adm-clerical': \"Occupation:Low-income\",\n",
    "            'Transport-moving|Craft-repair' : 'Occupation:Mid-income',\n",
    "            'Sales|Armed-Forces|Tech-support|Protective-serv|Protective-serv' :'Occupation:High-income',\n",
    "            'Prof-specialty|Exec-managerial' : 'Occupation:Very-High-income'}, regex=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    df_nouvelle_data = pd.read_csv('files/nouvelle_data.csv')\n",
    "    df_nouvelle_data = drop_columns(df_nouvelle_data)\n",
    "    df_nouvelle_data = get_classes(df_nouvelle_data)\n",
    "    df_nouvelle_data = regroupement_V2(df_nouvelle_data)\n",
    "    X = pd.get_dummies(df_nouvelle_data)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression_V3(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Crée le Logit et l'entraîne grâce aux datasets d'entraînement\n",
    "    Avec les hyper-paramètres du GridSearchCV\n",
    "    \"\"\"\n",
    "    model = LogisticRegression(random_state = 42, \n",
    "                               C=10,\n",
    "                               fit_intercept=False,\n",
    "                               max_iter = 500,\n",
    "                               tol = 0.0001,\n",
    "                               solver='lbfgs').fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def DecisionTree_V3(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Créer l'arbre de décision et l'entraîne grâce aux datasets d'entraînement\n",
    "    Un seul hyper-paramètres : max_depth = 7\n",
    "    \"\"\"\n",
    "    model = tree.DecisionTreeClassifier(max_depth=13,\n",
    "                                        max_leaf_nodes=63,\n",
    "                                        min_samples_leaf=20, \n",
    "                                        min_samples_split=60).fit(X_train, y_train)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_V2 = pd.read_csv('files/data/data_V2.csv')\n",
    "X_train, X_test, y_train, y_test = Create_Train_Test(df_V2)\n",
    "model_decision_tree = DecisionTree_V3(X_train, y_train)\n",
    "X_nouvelle_data = Create_X_nouvelle_data()\n",
    "predict_tree = model_decision_tree.predict(X_nouvelle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderlunel/Documents/LILLE/Master/MasterSIAD/M2/Scoring/Etude de cas/venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "df_V2 = pd.read_csv('files/data/data_V2.csv')\n",
    "X_train, X_test, y_train, y_test = Create_Train_Test(df_V2)\n",
    "model_logit = Logistic_Regression_V3(X_train, y_train)\n",
    "X_nouvelle_data = Create_X_nouvelle_data()\n",
    "predict_logit = model_logit.predict(X_nouvelle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Logit >50k  Logit <=50k\n",
      "Tree >50k         1086         1102\n",
      "Tree <=50k         345         7148\n",
      "            Logit >50k  Logit <=50k\n",
      "Tree >50k    11.217849    11.383122\n",
      "Tree <=50k    3.563681    73.835348\n",
      "Pourcentage de prédictions identiques: 85.05%\n",
      "Pourcentage de prédictions différentes 14.95%\n",
      "Pourcentage de prédictions >50k Logit 14.78%\n",
      "Pourcentage de prédictions >50k Logit 22.6%\n",
      "Nombre de prédictions >50k Logit 1431\n",
      "Nombre de prédictions >50k Tree 2188\n"
     ]
    }
   ],
   "source": [
    "predict = pd.concat([pd.Series(predict_tree), pd.Series(predict_logit)], axis=1)\n",
    "predict.columns = ['Tree', 'Logit']\n",
    "\n",
    "conf_matrix = confusion_matrix(predict['Tree'], predict['Logit'], labels=[True, False])\n",
    "df_conf_matrix = pd.DataFrame(conf_matrix, index=['Tree >50k', 'Tree <=50k'], columns=['Logit >50k', 'Logit <=50k'])\n",
    "prc_df_conf_matrix = df_conf_matrix/len(predict)*100\n",
    "print(df_conf_matrix)\n",
    "print(prc_df_conf_matrix)\n",
    "print(\"Pourcentage de prédictions identiques:\", f\"{round(prc_df_conf_matrix['Logit >50k']['Tree >50k']+prc_df_conf_matrix['Logit <=50k']['Tree <=50k'], 2)}%\")\n",
    "print(\"Pourcentage de prédictions différentes\", f\"{round(prc_df_conf_matrix['Logit <=50k']['Tree >50k']+prc_df_conf_matrix['Logit >50k']['Tree <=50k'], 2)}%\")\n",
    "print(\"Pourcentage de prédictions >50k Logit\", f\"{round(df_conf_matrix['Logit >50k'].sum()/len(predict)*100, 2)}%\")\n",
    "print(\"Pourcentage de prédictions >50k Logit\", f\"{round(df_conf_matrix.T['Tree >50k'].sum()/len(predict)*100, 2)}%\")\n",
    "print(\"Nombre de prédictions >50k Logit\", df_conf_matrix['Logit >50k'].sum())\n",
    "print(\"Nombre de prédictions >50k Tree\", df_conf_matrix.T['Tree >50k'].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
