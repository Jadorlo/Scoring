{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc, roc_curve, matthews_corrcoef\n",
    "import argparse\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Train_Test(df):\n",
    "    \"\"\"\n",
    "    Créer les dataframes de test et d'entraînement \n",
    "    \"\"\"\n",
    "    y = df.pop('income')\n",
    "    y = pd.get_dummies(y)['>50K']\n",
    "    X = pd.get_dummies(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0, stratify=y)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_X_nouvelle_data():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def drop_columns(df):\n",
    "        \"\"\"\n",
    "        Drop les colonnes fnlwgt, educational-nul car inutiles\n",
    "        \"\"\"\n",
    "        df.drop([\"fnlwgt\", \"educational-num\"], inplace=True, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def get_opti_classes(df):\n",
    "        \"\"\"\n",
    "        Applique les classes optimisées générées par le fichier optimisation_classes.py\n",
    "        \"\"\"\n",
    "\n",
    "        def classifier(classes, var):\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # Parcourir chaque intervalle dans la liste\n",
    "            for interval in classes:\n",
    "                # Extraire l'intervalle sous forme de chaîne\n",
    "                interval_str = interval[0]\n",
    "\n",
    "                # Extraire les bornes inférieure et supérieure en enlevant les parenthèses et crochets\n",
    "                lower_bound, upper_bound = interval_str[1:-1].split(', ')\n",
    "                \n",
    "                # Convertir les bornes en float\n",
    "                lower_bound = float(lower_bound)\n",
    "                upper_bound = float(upper_bound)\n",
    "\n",
    "                # Vérifier si l'âge est dans cet intervalle\n",
    "                if lower_bound < var <= upper_bound:\n",
    "                    return interval_str\n",
    "                \n",
    "        serie_age = pd.read_csv('files/classes_opti/classes_opti_alexander_age.csv')\n",
    "        serie_hpw = pd.read_csv('files/classes_opti/classes_opti_alexander_hours-per-week.csv')\n",
    "        \n",
    "        for serie in [serie_age, serie_hpw]:\n",
    "            classes = serie.value_counts().index\n",
    "            df_classes = df[serie.columns[0]].apply(lambda x : classifier(classes, x))\n",
    "            df.drop(serie.columns[0], inplace=True, axis=1)\n",
    "            df = pd.concat([df, df_classes], axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def regroupement_V2(df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Regroupement des personnes d'ethnies autre que blanc et noir en \"Other_race\"\n",
    "        df['race'] = df['race'].replace(\n",
    "            {'Asian-Pac-Islander|Amer-Indian-Eskimo|Other': 'Other_race'}, regex=True)\n",
    "\n",
    "        # Regroupement des personnes d'origines différentes en une cartégorie \"Other\"\n",
    "        df['native-country'] = df['native-country'].replace(\n",
    "            {'^(?!United-States$).+': 'Other_country'}, regex=True)\n",
    "\n",
    "        # Regroupement des personnes mariés mais avec un conjoint dans l'armé avec les personnes mariés mais avec un conjoint absent pour diverse raisons\n",
    "        df['marital-status'] = df['marital-status'].replace(\n",
    "            {'Married-AF-spouse|Divorced|Widowed|Separated|Married-spouse-absent': 'Alone'}, regex = True)\n",
    "\n",
    "        df['education'] = df['education'].replace(\n",
    "            {'Preschool|1st-4th|5th-6th|7th-8th|9th|10th|11th|12th': 'Low-education',\n",
    "            'Doctorate|Prof-school': \"Graduation\",\n",
    "            \"Assoc-acdm|Assoc-voc\" : 'Assoc'}, regex=True)\n",
    "\n",
    "        df['workclass'] = df['workclass'].replace(\n",
    "            {'Local-gov|State-gov': 'State-Local-gov',\n",
    "            'Never-worked|Without-pay|\\?' : 'No_income_or_unknown'}, regex=True)\n",
    "\n",
    "        df['occupation'] = df['occupation'].replace(\n",
    "            {'\\?|Priv-house-serv|Other-service|Handlers-cleaners': 'Occupation:Very-Low-income',\n",
    "            'Farming-fishing|Machine-op-inspct|Adm-clerical': \"Occupation:Low-income\",\n",
    "            'Transport-moving|Craft-repair' : 'Occupation:Mid-income',\n",
    "            'Sales|Armed-Forces|Tech-support|Protective-serv|Protective-serv' :'Occupation:High-income',\n",
    "            'Prof-specialty|Exec-managerial' : 'Occupation:Very-High-income'}, regex=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    df_nouvelle_data = pd.read_csv('files/nouvelle_data.csv')\n",
    "    df_nouvelle_data = drop_columns(df_nouvelle_data)\n",
    "    df_nouvelle_data = get_opti_classes(df_nouvelle_data)\n",
    "    df_nouvelle_data = regroupement_V2(df_nouvelle_data)\n",
    "    X = pd.get_dummies(df_nouvelle_data)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression_V3(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Crée le Logit et l'entraîne grâce aux datasets d'entraînement\n",
    "    Avec les hyper-paramètres du GridSearchCV\n",
    "    \"\"\"\n",
    "    model = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def DecisionTree_V2(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Créer l'arbre de décision et l'entraîne grâce aux datasets d'entraînement\n",
    "    Un seul hyper-paramètres : max_depth = 7\n",
    "    \"\"\"\n",
    "    model = tree.DecisionTreeClassifier(max_depth=13,\n",
    "                                        max_leaf_nodes=63,\n",
    "                                        min_samples_leaf=20, \n",
    "                                        min_samples_split=60).fit(X_train, y_train)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n",
      "[7493 2188]\n",
      "22.6%\n"
     ]
    }
   ],
   "source": [
    "df_V2 = pd.read_csv('files/data/data_V2.csv')\n",
    "X_train, X_test, y_train, y_test = Create_Train_Test(df_V2)\n",
    "model_decision_tree = DecisionTree_V2(X_train, y_train)\n",
    "X_nouvelle_data = Create_X_nouvelle_data()\n",
    "unique, count = np.unique(model_decision_tree.predict(X_nouvelle_data), return_counts=True)\n",
    "print(unique)\n",
    "print(count)\n",
    "print(f\"{round(count[1]/sum(count)*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderlunel/Documents/LILLE/Master/MasterSIAD/M2/Scoring/Etude de cas/venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n",
      "[8416 1265]\n",
      "13.07%\n"
     ]
    }
   ],
   "source": [
    "df_V2 = pd.read_csv('files/data/data_V2.csv')\n",
    "X_train, X_test, y_train, y_test = Create_Train_Test(df_V2)\n",
    "model_logit = Logistic_Regression_V3(X_train, y_train)\n",
    "X_nouvelle_data = Create_X_nouvelle_data()\n",
    "unique, count = np.unique(model_logit.predict(X_nouvelle_data), return_counts=True)\n",
    "print(unique)\n",
    "print(count)\n",
    "print(f\"{round(count[1]/sum(count)*100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
